---
title: "hw6"
output: html_document
date: "2022-11-18"
---

Set up for hw 6
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidymodels)
library(tidyverse)
library(dplyr)
library(corrr)
library(corrplot)
library(ggplot2)
library(discrim)
library(klaR)
library(glmnet)
library(ISLR)
library(rpart.plot)
library(vip)
library(randomForest)
library(xgboost)
library(ranger)
library(tidyr)
tidymodels_prefer()
pokemon <- read.csv("data/Pokemon.csv")
set.seed(1234)
```


Exercise 1
```{r}
library(janitor)
pokemon <- pokemon %>%
  clean_names()
#filter out rarer classes
pokemon_filter <- pokemon %>%
  filter(type_1 == c("Bug", "Fire", "Grass", "Normal", "Water", "Psychic"))
#convert type_1 and legendary to factors plus generation
pokemon_factor <- pokemon_filter %>%
  mutate(type_1 = factor(type_1), 
         legendary = factor(legendary), 
         generation = factor(generation))
#initial_split with percentage; stratify type_1
pokemon_split <- initial_split(pokemon_factor, 
                               prop = 0.7, 
                               strata = type_1)
pokemon_split

pokemon_train <- training(pokemon_split)
pokemon_test <- testing(pokemon_split)
#v-fold with v=5; strata = type_1
pokemon_folds <- vfold_cv(pokemon_train, v = 5, 
                          strata = type_1)
pokemon_folds
#recipe
pokemon_recipe <-
  recipe(type_1 ~ legendary + generation + sp_atk + attack + speed + defense + 
           hp + sp_def, data = pokemon_train) %>%
  step_dummy(legendary, generation) %>% 
  #step_normalize(all_predictors())
  step_center(all_predictors()) %>% 
  step_scale(all_predictors())
pokemon_recipe

```


Exercise 2
```{r}
#EXERCISE 2: correlation matrix
#library(corrplot)
pokemon_train %>%
  select_if(is.numeric) %>%
  select(-x) %>%
  cor(use = "complete.obs") %>%
  corrplot(type = "lower", diag = FALSE, method = "color")
```


I decided to exclude x since it isn't the main focus in analyzing the pokemon. 
The relationships I notice is that all variables have no relation to highly positively correlated relationship with each other. This makes sense to me as highly leveled pokemon individuals would have a higher attack, defense, and all the other varaibles (sp_atk, sp_def, hp) than lower level pokemon individuals, which will cause them to also have a higher total score. 


Exercise 3
```{r}
#EXERCISE 3: decision tree
tree_spec <- decision_tree() %>%
  set_engine("rpart")
class_tree_spec <- tree_spec %>%
  set_mode("classification") %>%
  set_args(cost_complexity = tune())

class_tree_wf <- workflow() %>%
  add_model(class_tree_spec) %>% 
  add_recipe(pokemon_recipe) #recipe?

param_grid <- grid_regular(cost_complexity(range = c(-3, -1)), levels = 10)

```
Exercise 3 con. 
```{r, eval = FALSE}
#tune_res <- tune_grid(
  #class_tree_wf, 
  #resamples = pokemon_folds, 
  #grid = param_grid, 
  #metrics = metric_set(roc_auc)
#)

write_rds(tune_res, file = "decision-tree-res.rds")

decision_tree <- read_rds(file = "decision-tree-res.rds")
autoplot(decision_tree)
```

I see that the cost-complexity parameter always maintained below a 0.610 roc_auc, but once the cost-complexity parameter reached around 0.035 (between 0.01 and 0.065), it dropped down to below 0.59 roc_auc and rose slightly when the cost-complexity parameter was 0.1 to a value of almost 0.5975 roc_auc. Therefore a single decision tree performed better with a smaller complexity penalty as having a value too big may overprune the tree. 


```{r}
#EXERCISE 4: roc_auc
library(yardstick)
decision_tree <- read_rds(file = "decision-tree-res.rds")

decision_roc <- decision_tree %>%
  collect_metrics() %>%
  arrange(desc(mean)) %>%
  slice(1)
decision_roc
```

The roc_auc of my best-performing pruned decision tree on the folds is 0.001 and estimates of the roc_auc curve are under 0.61. 


Exercise 5
```{r}
#EXERCISE 5 prt 1: rpart.plot
collect_metrics(decision_tree)
best_penalty <- select_best(decision_tree, metric = "roc_auc")

tree_final <- finalize_workflow(class_tree_wf, best_penalty)

tree_final_fit <- fit(tree_final, data = pokemon_train)

tree_final_fit %>%
  extract_fit_engine() %>%
  rpart.plot()
```

Exercise 5 prt 6 ("Exercise 6")
```{r}
#EXERCISE 5: random forest model and wrkflow
bagging_spec <- rand_forest() %>%
  set_engine("ranger", importance = 'impurity') %>%
  set_mode("classification") %>%
  set_args(mtry = tune(),
           trees = tune(),
           min_n = tune())

class_tree_wf2 <- workflow() %>%
  add_model(bagging_spec) %>%
  add_recipe(pokemon_recipe)
param_grid2 <- grid_regular(mtry(range = c(1, 8)), trees(range = c(200, 1000)),
                            min_n(range = c(5, 20)), levels = 8)
```

The mtry is the number of randomly sampled variables for each split. Trees are the number of trees per forest. Min_n is the minimum number of predictors at each split. Mtry should not be smaller than 1 or larger than 8 as you would be using more predictors than provided. Mtry = 8 means that each decision tree has all the available predictors for each split.

Exercise 6 or "Exercise 7"
```{r, eval=FALSE}
#EXERCISE 6: roc_auc as metric -- takes a few minutes to run
#tune_res2 <- tune_grid(
  #class_tree_wf2, 
  #resamples = pokemon_folds, 
  #grid = param_grid2, 
  #metrics = metric_set(roc_auc)
#)

write_rds(tune_res2, file = "rand-forest-res.rds")
rand_tree <- read_rds(file = "rand-forest-res.rds")
autoplot(rand_tree)
```

I observe that the roc_auc would increase as the minimal node size increases. The values of hyperparameters seem to yield the best performance were 15, 17, and 20.


Exercise 7 ("Exercise 8")
```{r}
#roc_auc of random forest model on folds
rand_tree <- read_rds(file = "rand-forest-res.rds")
rand_roc <- rand_tree %>%
  collect_metrics() %>%
  arrange(desc(mean)) %>%
  slice(1)
```

The roc_auc of my best-performing random forest model on the folds is 0.702. 

Exercise 8 ("Exercise 9")
```{r}
#EXERCISE 8: vip()
collect_metrics(rand_tree)
best_penalty2 <- select_best(rand_tree, metric = "roc_auc")

tree_final2 <- finalize_workflow(class_tree_wf2, best_penalty2)

tree_final_fit2 <- fit(tree_final2, data = pokemon_train)
vip(extract_fit_engine(tree_final_fit2))
```

The variables that were the most useful were sp_atk, speed, and hp. The variables that were the least useful were generation_X3, generation_X2, legendary_True, and generation_X6. These are the results I expected. 


Exercise 9 ("Exercise 10")
```{r, eval = FALSE}
#EXERCISE 9: boosted tree model
boost_spec <- boost_tree() %>%
  set_engine("xgboost") %>%
  set_mode("classification") %>%
  set_args(trees = tune())

class_tree_wf3 <- workflow() %>%
  add_model(boost_spec) %>%
  add_recipe(pokemon_recipe)

param_grid3 <- grid_regular(trees(range = c(10, 2000)),levels = 10)

#tune_res3 <- tune_grid(
  #class_tree_wf3, 
  #resamples = pokemon_folds, 
  #grid = param_grid3, 
  #metrics = metric_set(roc_auc)
#)

write_rds(tune_res3, file = "boosted-forest-res.rds")
boost_tree <- read_rds(file = "boosted-forest-res.rds")
autoplot(boost_tree)
```

I observe that the highest roc_auc was when the number of trees was around 894 trees with almost 0.666 roc_auc. 

Exercise 9 ("Exercise 10") con.
```{r}
#boosted tree model and workflow with roc_auc
boost_tree <- read_rds(file = "boosted-forest-res.rds")
boost_roc <- boost_tree %>%
  collect_metrics() %>%
  arrange(desc(mean)) %>%
  slice(1)
```
The roc_auc of my best-performing boosted tree model on the folds was 0.666. 

Exercise 10 ("Exercise 11")
```{r}
set.seed(1234)
result <- bind_rows(decision_roc, rand_roc, boost_roc) %>%
  tibble() %>%
  mutate(model = c('pruned tree model', 'random forest model', 
                   'boost tree model'), 
         .before = .metric)
result
#random forest did best:

collect_metrics(rand_tree)
final_penalty <- select_best(rand_tree)

final_tree <- finalize_workflow(class_tree_wf2, final_penalty)

final_fit <- fit(final_tree, data = pokemon_test)
final_auc_roc <- augment(final_fit, new_data = pokemon_test) %>%
  select(type_1, starts_with(".pred")) %>%
  roc_auc(type_1, .pred_Bug:.pred_Water) 
final_auc_roc #.964
```


```{r}
roc_auc2 <- augment(final_fit, new_data = pokemon_test) %>%
  select(type_1, starts_with(".pred")) 
final_fit2 <- roc_auc2 %>%
  roc_curve(type_1, .pred_Bug:.pred_Water)
ggplot2::autoplot(final_fit2)
```

```{r}
fit_3 <- augment(final_fit, new_data = pokemon_test) %>%
  conf_mat(truth = type_1, estimate = .pred_class)

autoplot(fit_3, type = "heatmap")
```

My model was good at predicting Bug, Psychic, Fire, Normal, and Grass. However, it was the worst at predicting Water.
---
title: "hw4"
output: html_document
date: "2022-11-03"
---

prep for hw 4
```{r}
library(tidymodels)
library(tidyverse)
library(dplyr)
#library(corrr)
library(ggplot2)
library(discrim)
library(klaR)
tidymodels_prefer()
titanic <- read.csv("data/titanic.csv") %>% 
  mutate(survived = factor(survived, levels = c("Yes", "No")))
titanic$pclass <- as.factor(titanic$pclass)
set.seed(1234)
```


Question 1
```{r}
titanic_split <- initial_split(titanic, 
                               prop = 0.80, 
                               strata = survived)
titanic_split

train <- training(titanic_split)
test <- testing(titanic_split)
```


179 observations for the testing data set and 712 observations for the training data set. In adding them up, it makes a total of 891 observations, which is an appropriate number of observations.

*INSERT RECIPE BELOW* FROM HW 3
```{r}
titanic_recipe <-
  recipe(survived ~ pclass+sex+age+sib_sp+parch+fare, data = train) %>%
  step_impute_linear(age,
                     impute_with = imp_vars(sib_sp, parch, fare)) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_interact(terms = ~starts_with("sex"):fare + age:fare)
titanic_recipe
```


Question 2
```{r}
titanic_folds <- vfold_cv(train, v = 10)
titanic_folds
```

Question 3
In Question 2, we are splitting our data in 10 roughly equal groups (which is our training data). K-fold validation is finding the best degree polynomial regression model for our (training) data set. We should use this, rather than simply fitting and testing models on the entire training set we want to see which model best fits our data on ten separate pieces of data, rather than just guessing. If we did use the entire training set, we would use the bootstrap method. 

Question 4
```{r}
log_model <- logistic_reg() %>%
  set_mode("classification") %>%
  set_engine("glm")

log_wrkflow <- workflow() %>%
  add_model(log_model) %>%
  add_recipe(titanic_recipe)

lda_model <- discrim_linear() %>%
  set_mode("classification") %>%
  set_engine("MASS")

lda_wrkflow <- workflow() %>%
  add_model(lda_model) %>%
  add_recipe(titanic_recipe)

qda_model <- discrim_quad() %>%
  set_mode("classification") %>%
  set_engine("MASS")

qda_wrkflow <- workflow() %>%
  add_model(qda_model) %>%
  add_recipe(titanic_recipe)

```

I would have a total of 30 models across all folds that I would fitting to the data. 

Question 5
```{r, eval = FALSE}
log_fit <- fit_resamples(log_wrkflow, titanic_folds)
lda_fit <- fit_resamples(lda_wrkflow, titanic_folds)
qda_fit <- fit_resamples(qda_wrkflow, titanic_folds)

#getwd()
#save(log_fit, lda_fit, qda_fit, file = "mydata.rda")
#rm(log_fit, lda_fit, qda_fit)
load(file = "mydata.rda")
```


Question 6: mean and std errors
```{r}
log_fit <- fit_resamples(log_wrkflow, titanic_folds)
lda_fit <- fit_resamples(lda_wrkflow, titanic_folds)
qda_fit <- fit_resamples(qda_wrkflow, titanic_folds)

log_m <- collect_metrics(log_fit)[1,] #mean: 0.7865, std: 0.0133
lda_m <- collect_metrics(lda_fit)[1,] #mean: 0.7893, std: 0.01283
qda_m <- collect_metrics(qda_fit)[1,] #mean: 0.76559, std: 0.0177

rbind(c(log_m, lda_m, qda_m))
bind_rows(list(log_m, lda_m, qda_m))
```

Based on the means, linear discriminant analysis has the largest mean with one of the lowest standard errors, so it performs the best. 

Question 7
```{r}
fit_all <- fit(lda_wrkflow, train)
```

Question 8
```{r}
fit_test <- fit(lda_wrkflow, test)
lda_pred <- predict(fit_test, new_data = test, 
                    type = "class") %>%
  bind_cols(test %>% select(survived))

lda_acc <- lda_pred %>%
  accuracy(truth = survived, estimate = .pred_class)

lda_acc
```

This model has a testing accuracy of an estimated 0.7989, while the average accuracy across folds was around 0.7893. This means our newest model has a higher accuracy than the average accuracy across folds.

